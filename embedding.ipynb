{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading Test Data\n",
    "\n",
    "This dataset gives us ~42K text chunks to embed, each roughly a paragraph or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\on-prem_RAG_pgvector\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(description='', citation='', homepage='', license='', features={'doi': Value(dtype='string', id=None), 'chunk-id': Value(dtype='string', id=None), 'chunk': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'summary': Value(dtype='string', id=None), 'source': Value(dtype='string', id=None), 'authors': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'categories': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'comment': Value(dtype='string', id=None), 'journal_ref': Value(dtype='string', id=None), 'primary_category': Value(dtype='string', id=None), 'published': Value(dtype='string', id=None), 'updated': Value(dtype='string', id=None), 'references': [{'id': Value(dtype='string', id=None)}]}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='json', dataset_name='json', config_name='default', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=135573178, num_examples=41584, shard_lengths=None, dataset_name='json')}, download_checksums={'C:/code/on-prem_RAG_pgvector/data/train.jsonl': {'num_bytes': 152907501, 'checksum': None}}, download_size=152907501, post_processing_size=None, dataset_size=135573178, size_in_bytes=288480679)\n",
      "<class 'list'> 41584\n",
      "<class 'str'> 1258\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked/viewer/default/train?p=0\n",
    "# data = load_dataset(\n",
    "#     \"jamescalam/ai-arxiv-chunked\",\n",
    "#     split= \"train\")\n",
    "\n",
    "\n",
    "data = load_dataset(\"json\", data_files=\"data/train.jsonl\", split=\"train\")    \n",
    "print(data.info)\n",
    "print(type(data['chunk']),len(data['chunk'])) \n",
    "print(type(data['chunk'][0]),len(data['chunk'][0])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings\n",
    "\n",
    "create an embed function for each model -  return a list of vector embeddings from string list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hugginface; load embedding model & embedding function to use with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "from embedding_models.hugginface import load_LLM\n",
    "\n",
    "\n",
    "# model_id = \"intfloat/e5-base-v2\"\n",
    "model_id = \"C:/models/e5-base-v2\"\n",
    "\n",
    "model, tokenizer, device = load_LLM(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Vector Index\n",
    "\n",
    "Use this to build a Numpy array of embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding_models.hugginface import embed\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def VectorIndex_in_memory(chunks,tokenizer= None,model= None,device = None):\n",
    "    LEN = len(chunks)\n",
    "    batch_size = 256\n",
    "    for i in tqdm(range(0, LEN, batch_size)):\n",
    "        # start = time.time()   \n",
    "        chunk_batch = chunks[i:min(i+batch_size,LEN)].copy()\n",
    "        # embed current batch\n",
    "        embed_batch = embed(chunk_batch,tokenizer=tokenizer,model=model,device=device)\n",
    "        if i > 0: # add to existing np array if exists (otherwise create)\n",
    "            # embed_batch is the new batch of embeddings (same size as chunk_batch)\n",
    "            # arr = np.concatenate([arr, embed_batch.copy()])\n",
    "            arr += embed_batch.copy()\n",
    "        if i == 0:\n",
    "            arr = embed_batch.copy()\n",
    "        # print(f'batch time: {time.time() - start}')\n",
    "        if i>10:\n",
    "            break\n",
    "    print(i,len(arr) )   \n",
    "    return arr\n",
    "# arr = VectorIndex_in_memory(chunks = data[\"chunk\"],tokenizer=tokenizer,model=model,device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:21<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished writing Data to DB!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from add_data_to_db import VectorIndexUpdate\n",
    "VectorIndexUpdate(texts= data[\"chunk\"][:100],tokenizer=tokenizer,model=model,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the query mechanism, this is simply a cosine similarity calculation between a query vector and our arr vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy.linalg import norm\n",
    "\n",
    "# # convert chunks list to array for easy indexing\n",
    "# chunk_arr = np.array(chunks)\n",
    "\n",
    "# def query_in_memory(text: str, top_k: int=3) -> list[str]:\n",
    "#     # create query embedding\n",
    "#     xq = embed([f\"query: {text}\"])[0]\n",
    "#     # calculate cosine similarities\n",
    "#     sim = np.dot(arr, xq.T) / (norm(arr, axis=1)*norm(xq.T))\n",
    "#     # get indices of top_k records\n",
    "#     idx = np.argpartition(sim, -top_k)[-top_k:]\n",
    "#     docs = chunk_arr[idx]\n",
    "#     for d in docs.tolist():\n",
    "#         print(d)\n",
    "#         print(\"----------\")\n",
    "# # query_in_memory(\"why should I use llama 2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: why should I use llama 2?\n",
      "Similarity: 0.7536534335583621;\ttable ID: 324\tContent: little to no technical skill. While this might make our paper seem harmful, we believe the beneﬁts of\n",
      "publishing this attack far outweighs any potential harms.\n",
      "The ﬁrst reason the beneﬁts outweigh the harms is that, to the best of our knowledge, multimodal\n",
      "contrastive classiﬁers are not yet used in any security-critical situations. And so, at least today,\n",
      "we are not causing any direct harm by publishing the feasibility of these attacks. Unlike work on\n",
      "adversarial attacks, or indeed any other traditional area of computer security or cryptanalysis that\n",
      "develops attacks on deployed systems, the attacks in our paper can not be used to attack any system\n",
      "that exists right now.\n",
      "Compounding on the above, by publicizing the limitations of these classiﬁers early, we can prevent\n",
      "users in the future from assuming these classiﬁers are robust when they in fact are not. If we were\n",
      "to wait to publish the feasibility of these attacks, then organizations might begin to train contrastive\n",
      "classiﬁers for safety-critical situations not realizing the potential problems that may exist. Once\n",
      "contrastive classiﬁers begin to be used widely, the potential for harm only increases with time.\n",
      "Finally, by describing the feasibility of these attacks now, we maximize the time available for the\n",
      "Similarity: 0.7470251753954926;\ttable ID: 131\tContent: 10.0\n",
      "DoubleDunk\n",
      "0250500750Enduro\n",
      "100\n",
      "50\n",
      "0FishingDerby\n",
      "0102030Freeway\n",
      "100200300Frostbite\n",
      "02000040000Gopher\n",
      "250500750Gravitar\n",
      "10\n",
      "8\n",
      "6\n",
      "4\n",
      "IceHockey\n",
      "0200400600Jamesbond\n",
      "0500010000Kangaroo\n",
      "2000400060008000Krull\n",
      "02000040000KungFuMaster\n",
      "050100MontezumaRevenge\n",
      "100020003000MsPacman\n",
      "25005000750010000NameThisGame\n",
      "100\n",
      "0Pitfall\n",
      "20\n",
      "020Pong\n",
      "0500PrivateEye\n",
      "050001000015000Qbert\n",
      "25005000750010000Riverraid\n",
      "02000040000RoadRunner\n",
      "246Robotank\n",
      "050010001500Seaquest\n",
      "5001000SpaceInvaders\n",
      "02000040000StarGunner\n",
      "20\n",
      "15\n",
      "10\n",
      "Tennis\n",
      "30004000TimePilot\n",
      "0100200300Tutankham\n",
      "0100000200000UpNDown\n",
      "0 40M\n",
      "Frames0510Venture\n",
      "0 40M\n",
      "Frames50000100000150000VideoPinball\n",
      "0 40M\n",
      "Frames20004000WizardOfWor\n",
      "0 40M\n",
      "Frames0200040006000Zaxxon\n",
      "A2C\n",
      "ACER\n",
      "Similarity: 0.7439936140155935;\ttable ID: 1\tContent: I like to eat broccoli and bananas.\n",
      "\n",
      "finished reading Data from DB!\n"
     ]
    }
   ],
   "source": [
    "from vector_search import query\n",
    "texts = [\"why should I use llama 2?\"]\n",
    "query(texts, model, tokenizer, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usecases\n",
    "\n",
    "https://medium.com/@vladris/embeddings-and-vector-databases-732f9927b377\n",
    "\n",
    "https://medium.com/@vladris/n-shot-learning-f9bc0d670a41\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q&A solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "embeddings = {}\n",
    "\n",
    "for f in os.listdir('./racing'):\n",
    "    path = os.path.join('./racing', f)\n",
    "    with open(path, 'r') as f:\n",
    "        text = [f.read()]\n",
    "\n",
    "    embeddings[path] = embed(text,tokenizer=tokenizer,model=model,device=device)[0]\n",
    "   \n",
    "\n",
    "with open('embeddings.json', 'w+') as f:\n",
    "    json.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the Genosis Challenge Pod Racing race, there were several exhilarating and unforeseen events that shaped the final standings:\n",
      "\n",
      "Lightning Bolt's Electrodynamic Boost: Tira Suro, piloting the Lightning Bolt, had equipped the pod with a cutting-edge electrodynamic propulsion system. As the race began, Suro ingeniously synchronized the pod's engine with the planet's unique electromagnetic field, harnessing its energy to achieve an unprecedented burst of speed. This electrifying boost propelled the Lightning Bolt into an early lead, setting the stage for Suro's victory.\n",
      "\n",
      "Razor Blade's Risky Gambit: Kael Voss, the pilot of the Razor Blade, opted for a daring strategy to gain an advantage. Approaching a treacherous section filled with narrow rock formations, Voss executed a series of precise maneuvers, utilizing the Razor Blade's superior agility to navigate through the hazardous obstacles. Despite the risks involved, Voss's calculated moves allowed the Razor Blade to maintain a strong position, ultimately securing second place.\n",
      "\n",
      "Thunderbolt's Technical Glitch: Senn Kava, piloting the Thunderbolt pod, encountered an unexpected technical glitch during a crucial stage of the race. A malfunction in the pod's stabilization system caused Kava to lose control momentarily, resulting in a brief deviation from the racing line. Despite this setback, Kava's skillful recovery and determination enabled them to regain momentum and finish in third place.\n",
      "\n",
      "Crimson Fang's Thrilling Pursuit: Remy Thal, piloting the Crimson Fang, demonstrated exceptional perseverance and a never-say-die attitude throughout the race. Despite starting in a lower position, Thal showcased relentless determination, employing precise cornering techniques and exploiting gaps in the field to make a series of remarkable overtakes. Thal's tenacity ultimately earned them the fourth-place position.\n",
      "\n",
      "Shadow Racer's Unforeseen Obstacle: Vix Tor, the pilot of the Shadow Racer, encountered an unexpected obstacle during a crucial segment of the race. A sandstorm suddenly swept across the course, impairing visibility and causing Tor to momentarily lose control. The unforeseen challenge hampered Tor's progress, resulting in a drop to fifth place. Despite the setback, Tor exhibited admirable skill in maneuvering through the turbulent sands and completing the race.\n",
      "\n",
      "These captivating and unpredictable occurrences made the Genosis Challenge Pod Racing race an exhilarating spectacle, showcasing the racers' skills, adaptability, and resilience in the face of unexpected obstacles.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "embeddings = json.load(open('embeddings.json', 'r'))\n",
    "\n",
    "def cosine_distance(a, b):\n",
    "    return 1 - sum([a_i * b_i for a_i, b_i in zip(a, b)]) / (\n",
    "        sum([a_i ** 2 for a_i in a]) ** 0.5 * sum([b_i ** 2 for b_i in b]) ** 0.5)\n",
    "def nearest_embedding(embedding):\n",
    "    nearest, nearest_distance = None, 1\n",
    "\n",
    "    for path, embedding2 in embeddings.items():\n",
    "        distance = cosine_distance(embedding, embedding2)\n",
    "        if distance < nearest_distance:\n",
    "            nearest, nearest_distance = path, distance\n",
    "\n",
    "    return nearest#name of nearest file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i=0\n",
    "while i<1:\n",
    "    i+=1\n",
    "    # prompt = input('user: ')\n",
    "    prompt = 'What happened to Senn Kava during the Genosis Challenge?'\n",
    "    if prompt == 'exit':\n",
    "        break\n",
    "    \n",
    "    # find best context for prompt\n",
    "    context = nearest_embedding(embed([prompt],tokenizer=tokenizer,model=model,device=device)[0])\n",
    "    \n",
    "    # open context - and ask new model question on given context\n",
    "    data = open(context, 'r').read()\n",
    "    print(data)\n",
    "    # message = chat.completion(\n",
    "    #     {'data': data, 'prompt': prompt}).choices[0].message\n",
    "    # print(f'{message.role}: {message.content}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LlamaCpp' from 'langchain_community' (c:\\code\\on-prem_RAG_pgvector\\.venv\\Lib\\site-packages\\langchain_community\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaCpp\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# openai.api_key = None\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# if openai.api_key is None:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     raise Exception('OPENAI_API_KEY not set')\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert_params\u001b[39m(string, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LlamaCpp' from 'langchain_community' (c:\\code\\on-prem_RAG_pgvector\\.venv\\Lib\\site-packages\\langchain_community\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "from langchain_community import LlamaCpp\n",
    "\n",
    "\n",
    "# openai.api_key = None\n",
    "# if openai.api_key is None:\n",
    "#     raise Exception('OPENAI_API_KEY not set')\n",
    "\n",
    "def insert_params(string, **kwargs):\n",
    "    pattern = r\"{{(.*?)}}\"\n",
    "    matches = re.findall(pattern, string)\n",
    "    for match in matches:\n",
    "        replacement = kwargs.get(match.strip())\n",
    "        if replacement is not None:\n",
    "            string = string.replace(\"{{\" + match + \"}}\", replacement)\n",
    "    return string\n",
    "\n",
    "class ChatTemplate:\n",
    "    def __init__(self, template):\n",
    "        self.template = template\n",
    "\n",
    "    def from_file(template_file):\n",
    "        with open(template_file, 'r') as f:\n",
    "            template = json.load(f)\n",
    "        return ChatTemplate(template)\n",
    "\n",
    "    def completion(self, parameters):\n",
    "        instance = copy.deepcopy(self.template)\n",
    "        for item in instance['messages']:\n",
    "            item['content'] = insert_params(item['content'], **parameters)\n",
    "        # return openai.ChatCompletion.create(\n",
    "        #     model='gpt-3.5-turbo',\n",
    "        #     **instance)\n",
    "        return \n",
    "        model_path = 'C:/models/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf'\n",
    "\n",
    "        llm = LlamaCpp(model_path = model_path, temperature=0.5, max_tokens=500, top_p=0.9, top_k=50,)\n",
    "\n",
    "        return  llm.invoke(instance)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chat = ChatTemplate( template=\n",
    "    {'messages': [{'role': 'system', 'content': 'You are a Q&A AI.'},\n",
    "                  {'role': 'system', 'content': 'Here are some facts that can help you answer the following question: {{data}}'},\n",
    "                  {'role': 'user', 'content': '{{prompt}}'}]\n",
    "     })\n",
    "message = chat.completion(\n",
    "    {'data': data, 'prompt': prompt})\n",
    "# .choices[0].message\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LlamaCpp' from 'langchain_community' (c:\\code\\on-prem_RAG_pgvector\\.venv\\Lib\\site-packages\\langchain_community\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/models/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaCpp\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m LlamaCpp(model_path \u001b[38;5;241m=\u001b[39m model_path, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LlamaCpp' from 'langchain_community' (c:\\code\\on-prem_RAG_pgvector\\.venv\\Lib\\site-packages\\langchain_community\\__init__.py)"
     ]
    }
   ],
   "source": [
    "model_path = 'C:/models/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf'\n",
    "from langchain_community import LlamaCpp\n",
    "from llama_cpp import Llama\n",
    "llm = LlamaCpp(model_path = model_path, temperature=0.5, max_tokens=500, top_p=0.9, top_k=50,)\n",
    "# prompt = \n",
    "\n",
    "\n",
    "text = \"What is the capital of France?\"\n",
    "response = llm.invoke(prompt.format(text = text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
